# Document Assistant ğŸ“„

A **100% local, privacy-first** document assistant that helps you understand your documents using AI and RAG (Retrieval-Augmented Generation). Built with Next.js and [LlamaFarm](https://docs.llamafarm.dev), all PDF processing happens entirely in your browser â€“ your documents never leave your device.

## âœ¨ Key Features

- **ğŸ”’ Complete Privacy** â€“ PDFs parsed client-side, no server uploads, data stays on your device
- **ğŸ¤– Multi-Hop Agentic RAG** â€“ AI orchestrates query generation, knowledge retrieval, and synthesis
- **ğŸ“š Flexible Knowledge Base** â€“ Support for text and PDF documents with semantic chunking
- **âš¡ Efficient AI Architecture** â€“ Fast model for queries, capable model for comprehensive responses
- **ğŸ’¬ Streaming Chat Interface** â€“ Real-time responses with citations and sources
- **ğŸ“„ Smart Document Analysis** â€“ Semantic chunking with context awareness
- **âš™ï¸ Configurable Retrieval** â€“ Adjust RAG top-k, score thresholds, and local document usage
- **ğŸ¨ Modern UI** â€“ Built with shadcn/ui, Tailwind CSS, and responsive design

---

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [How It Works](#-how-it-works)
- [Detailed Setup](#-detailed-setup)
  - [Prerequisites](#1-prerequisites)
  - [Install AI Models](#2-install-ai-models)
  - [Set Up LlamaFarm](#3-set-up-llamafarm)
  - [Add Your Documents](#4-add-your-documents)
  - [Create & Process Dataset](#5-create-and-process-dataset)
  - [Configure Frontend](#6-configure-frontend)
  - [Run Application](#7-run-application)
- [Architecture](#-architecture)
- [Usage Guide](#-usage-guide)
- [Development](#-development)
- [Troubleshooting](#-troubleshooting)
- [Privacy & Security](#-privacy--security)
- [License](#-license)

---

## ğŸš€ Quick Start

For experienced developers who want to get running quickly:

```bash
# 1. Install Docker
# macOS: https://docs.docker.com/desktop/install/mac-install/
brew install --cask docker
# OR download from: https://www.docker.com/products/docker-desktop

# Linux:
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Windows: Download from https://www.docker.com/products/docker-desktop

# 2. Install Ollama
# Download from: https://ollama.com/download
# macOS:
brew install ollama
# Linux:
curl -fsSL https://ollama.com/install.sh | sh
# Windows: Download installer from https://ollama.com/download

# Configure Ollama context window (important!)
# Open Ollama â†’ Settings â†’ Advanced â†’ Set context window to 32768+

# 3. Install LlamaFarm CLI
curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash
# Windows: Download lf.exe from https://github.com/llama-farm/llamafarm/releases

# 4. Pull AI models (1.4GB total, 5-15 min)
ollama pull gemma3:1b
ollama pull qwen3:1.7b
ollama pull nomic-embed-text

# 5. Initialize LlamaFarm
cd "Plain App"
lf init
lf start  # May take a few minutes on first run (downloads Docker images)

# 6. (Optional) Add and process your documents
# Place your .txt or .pdf files in a ./data directory
lf datasets add documents -s document_processor -b document_db
lf datasets ingest documents ./data/*.txt ./data/*.pdf
lf datasets process documents

# 7. Configure & run frontend
cp .env.local.example .env.local
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

---

## ğŸ” How It Works

This application combines **client-side PDF processing** with **server-side RAG** to provide intelligent document assistance:

1. **Upload Documents** â€“ Drop PDFs in your browser (parsed locally using pdf.js)
2. **Local Processing** â€“ Documents are chunked semantically in your browser
3. **Smart Retrieval** â€“ Queries are sent to LlamaFarm's RAG system with:
   - Local document chunks (if uploaded)
   - Vector database content (if configured)
   - Multi-hop reasoning for complex queries
4. **AI Response** â€“ Streaming responses with citations and sources
5. **Complete Privacy** â€“ Your documents never leave your device unless you explicitly add them to the LlamaFarm database

---

## ğŸ“– Detailed Setup

### 1. Prerequisites

- **Docker Desktop** â€“ For running LlamaFarm services
- **Node.js 18+** â€“ For the Next.js frontend
- **Ollama** â€“ For running local AI models
- **LlamaFarm CLI** â€“ For RAG orchestration

### 2. Install AI Models

Pull the required models (total ~1.4GB):

```bash
ollama pull gemma3:1b      # Fast query model
ollama pull qwen3:1.7b     # Capable reasoning model
ollama pull nomic-embed-text  # Embeddings model
```

**Important**: Configure Ollama's context window:
- Open Ollama settings
- Navigate to Advanced
- Set context window to at least 32768

### 3. Set Up LlamaFarm

Initialize LlamaFarm in the project directory:

```bash
cd "Plain App"
lf init
lf start
```

This starts:
- ChromaDB (vector database)
- LlamaFarm API server
- Background workers

### 4. Add Your Documents

You have two options for adding documents:

**Option A: Upload via Browser (Temporary)**
- Simply drag and drop PDFs into the web interface
- Documents are processed locally and not persisted
- Great for one-off queries

**Option B: Add to Knowledge Base (Persistent)**
```bash
# Create a data directory
mkdir -p data

# Add your documents (PDF or TXT)
cp /path/to/your/documents/*.pdf data/
cp /path/to/your/documents/*.txt data/
```

### 5. Create & Process Dataset

If you want persistent knowledge base:

```bash
# Create and ingest dataset
lf datasets add documents -s document_processor -b document_db
lf datasets ingest documents ./data/*.txt ./data/*.pdf

# Process documents (creates embeddings)
lf datasets process documents
```

### 6. Configure Frontend

```bash
cp .env.local.example .env.local
npm install
```

Edit `.env.local` if needed (defaults should work):
```env
NEXT_PUBLIC_LF_BASE_URL=http://localhost:8000
NEXT_PUBLIC_LF_NAMESPACE=default
NEXT_PUBLIC_LF_PROJECT=plain-app-project
NEXT_PUBLIC_LF_MODEL=default
NEXT_PUBLIC_LF_DATABASE=document_db
```

### 7. Run Application

```bash
npm run dev
```

Visit [http://localhost:3000](http://localhost:3000)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js App   â”‚  â† Upload PDFs (client-side parsing)
â”‚   (Browser)     â”‚  â† Chat interface
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTP API
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LlamaFarm API  â”‚  â† RAG orchestration
â”‚   (localhost)   â”‚  â† Query generation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â† Response synthesis
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChromaDB   â”‚  â”‚   Ollama    â”‚
â”‚   (Vectors)  â”‚  â”‚  (AI Models)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

- **Frontend (Next.js)**: PDF parsing, chat UI, local document management
- **LlamaFarm**: RAG orchestration, query generation, response synthesis
- **ChromaDB**: Vector storage for document embeddings
- **Ollama**: Local AI model inference

---

## ğŸ“š Usage Guide

### Basic Workflow

1. **Upload Documents** (optional)
   - Drag and drop PDFs into the upload area
   - Documents are chunked and ready for querying

2. **Ask Questions**
   - Type your question in the chat
   - Toggle "Use local docs" to include uploaded PDFs

3. **Review Responses**
   - See streaming AI responses
   - Check citations and sources
   - Adjust RAG settings if needed

### Settings

- **RAG Top-K**: Number of context chunks to retrieve (default: 6)
- **Use Local Docs**: Toggle to include/exclude uploaded PDFs
- **Score Threshold**: Minimum relevance score for retrieval (0.7)

---

## ğŸ› ï¸ Development

### Run Tests

```bash
npm test              # Unit tests
npm run test:e2e      # E2E tests (requires app running)
```

### Build for Production

```bash
npm run build
npm start
```

### Customize

- **Prompts**: Edit `llamafarm.yaml` â†’ `prompts` section
- **Models**: Change model names in `llamafarm.yaml` â†’ `runtime` section
- **Chunking**: Adjust in `llamafarm.yaml` â†’ `data_processing_strategies`
- **UI**: Components in `components/` directory

---

## ğŸ”§ Troubleshooting

### LlamaFarm won't start
```bash
lf stop
docker system prune -af
lf start
```

### Embeddings not working
- Verify `ollama pull nomic-embed-text` completed
- Check Ollama is running: `ollama list`
- Restart LlamaFarm: `lf restart`

### PDF parsing fails
- Ensure PDFs are not password-protected
- Check browser console for errors
- Try smaller PDFs first

### Slow responses
- Reduce RAG top-k in settings
- Use faster model (gemma3:1b)
- Check Docker resource allocation

---

## ğŸ”’ Privacy & Security

- **Client-side PDF parsing**: Documents are never uploaded to servers
- **Local AI models**: All inference happens on your machine
- **Optional cloud**: No cloud services required
- **Data control**: You own and control all data

When using "local docs" mode, PDFs are:
1. Parsed in your browser
2. Chunked locally
3. Sent to LlamaFarm (localhost) only as context

Your documents are **never** sent to external servers.

---

## ğŸ“„ License

MIT License - see LICENSE file for details.

**Disclaimer**: This is a general-purpose AI assistant. Always verify important information from authoritative sources.
